{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIIDETECT\n",
    "https://github.com/edwardcooper/piidetect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if some packages give trouble then downgrade those packages to what was available in 2019\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating fake profiles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 18304.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#from piidetect.fakepii import Fake_PII\n",
    "#fake_ = Fake_PII()\n",
    "#fake_.create_fake_profile(10)\n",
    "#train_labels, train_text, train_PII = fake_.create_pii_text_train(n_text = 5)#creating fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "word_embedding(workers=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "from piidetect.pipeline import word_embedding\n",
    "model = word_embedding(algo_name = \"word2vec\",size = 100, min_count = 1, workers =2)\n",
    "model.fit(data_train['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "from piidetect.pipeline import word_embedding\n",
    "model = word_embedding(algo_name = \"word2vec\",size = 100, min_count = 1, workers =2)\n",
    "model.fit(data_train['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "      <th>PII</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Property long both group. Pass office Apt. 100...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Apt. 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Occur attorney summer after heavy. Professor P...</td>\n",
       "      <td>Address</td>\n",
       "      <td>48756 Palmer Wells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apt. 100 Challenge investment forget continue ...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Apt. 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Continue across recognize exist fish. You Apt....</td>\n",
       "      <td>Address</td>\n",
       "      <td>Apt. 704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newspaper long everybody police. Service deter...</td>\n",
       "      <td>Address</td>\n",
       "      <td>7943 Daniel Row</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>State class memory kid sister to each. Poor fo...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Think tend herself ok reveal as.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Material total home offer who stage paper. Pla...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Article drug protect he free price same. Dinne...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Whose middle contain back ground top. Standard...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text   Labels  \\\n",
       "0    Property long both group. Pass office Apt. 100...  Address   \n",
       "1    Occur attorney summer after heavy. Professor P...  Address   \n",
       "2    Apt. 100 Challenge investment forget continue ...  Address   \n",
       "3    Continue across recognize exist fish. You Apt....  Address   \n",
       "4    Newspaper long everybody police. Service deter...  Address   \n",
       "..                                                 ...      ...   \n",
       "155  State class memory kid sister to each. Poor fo...     None   \n",
       "156                   Think tend herself ok reveal as.     None   \n",
       "157  Material total home offer who stage paper. Pla...     None   \n",
       "158  Article drug protect he free price same. Dinne...     None   \n",
       "159  Whose middle contain back ground top. Standard...     None   \n",
       "\n",
       "                    PII  \n",
       "0              Apt. 100  \n",
       "1    48756 Palmer Wells  \n",
       "2              Apt. 100  \n",
       "3              Apt. 704  \n",
       "4       7943 Daniel Row  \n",
       "..                  ...  \n",
       "155                None  \n",
       "156                None  \n",
       "157                None  \n",
       "158                None  \n",
       "159                None  \n",
       "\n",
       "[160 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new label with 0 and 1\n",
    "from piidetect.pipeline import binary_pii\n",
    "data_train['Target'] = data_train['Labels'].apply(binary_pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "      <th>PII</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Option protect 0070 Steven Lodge away just mem...</td>\n",
       "      <td>Address</td>\n",
       "      <td>0070 Steven Lodge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Large street quality subject figure such. 5103...</td>\n",
       "      <td>Address</td>\n",
       "      <td>5103 Alyssa Junction Suite 026 South Matthew, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84205 Jonathan Well Suite 322 West Kellyberg, ...</td>\n",
       "      <td>Address</td>\n",
       "      <td>84205 Jonathan Well Suite 322 West Kellyberg, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small any occur level. Option third his set bl...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Apt. 483</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Determine sit various quite as present. Watch ...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Suite 249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Give until smile necessary fly street.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Floor arrive reality image listen. Sister cond...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Now open war center with. Television picture w...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Evening close live now on there. Area put onto...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Approach my party. Reality table home maybe.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text   Labels  \\\n",
       "0    Option protect 0070 Steven Lodge away just mem...  Address   \n",
       "1    Large street quality subject figure such. 5103...  Address   \n",
       "2    84205 Jonathan Well Suite 322 West Kellyberg, ...  Address   \n",
       "3    Small any occur level. Option third his set bl...  Address   \n",
       "4    Determine sit various quite as present. Watch ...  Address   \n",
       "..                                                 ...      ...   \n",
       "235             Give until smile necessary fly street.     None   \n",
       "236  Floor arrive reality image listen. Sister cond...     None   \n",
       "237  Now open war center with. Television picture w...     None   \n",
       "238  Evening close live now on there. Area put onto...     None   \n",
       "239       Approach my party. Reality table home maybe.     None   \n",
       "\n",
       "                                                   PII  Target  \n",
       "0                                    0070 Steven Lodge       1  \n",
       "1    5103 Alyssa Junction Suite 026 South Matthew, ...       1  \n",
       "2    84205 Jonathan Well Suite 322 West Kellyberg, ...       1  \n",
       "3                                             Apt. 483       1  \n",
       "4                                            Suite 249       1  \n",
       "..                                                 ...     ...  \n",
       "235                                               None       0  \n",
       "236                                               None       0  \n",
       "237                                               None       0  \n",
       "238                                               None       0  \n",
       "239                                               None       0  \n",
       "\n",
       "[240 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n",
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/240 [00:00<?, ?it/s]/Users/priankaball/Desktop/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
      "100%|██████████| 240/240 [00:00<00:00, 12276.01it/s]\n",
      "100%|██████████| 240/240 [00:00<00:00, 170557.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('text_cleaning',\n",
       "                 <piidetect.pipeline.text_clean object at 0x7fe04dd4b6a0>),\n",
       "                ('word_embedding', word_embedding(workers=2)),\n",
       "                ('logit_clf_word2vec', LogisticRegression(max_iter=10000))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using logistic regression\n",
    "from piidetect.pipeline import word_embedding, text_clean\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit_clf_word2vec = LogisticRegression(solver = \"lbfgs\", max_iter = 10000)\n",
    "\n",
    "word2vec_pipe = Pipeline([('text_cleaning', text_clean()),\n",
    "                 (\"word_embedding\", word_embedding(algo_name = \"word2vec\", workers =2)),\n",
    "                 (\"logit_clf_word2vec\",logit_clf_word2vec)\n",
    "                ])\n",
    "                \n",
    "word2vec_pipe.fit(data_train[\"Text\"],data_train['Target'])#fitting the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameter tuning\n",
    "#this parts takes too long so we can skip\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from piidetect.pipeline import word_embedding, text_clean\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "\n",
    "#logit_clf_word2vec = LogisticRegression(solver = \"lbfgs\", max_iter = 10000)\n",
    "\n",
    "#pipe = Pipeline([('text_cleaning', text_clean()),\n",
    "#                 (\"word_embedding\", word_embedding( workers =2)),\n",
    "#                 (\"logit_clf_word2vec\",logit_clf_word2vec)\n",
    "#                ])\n",
    "\n",
    "\n",
    "#param_grid = {\n",
    "#    'word_embedding__algo_name':['word2vec', 'doc2vec','fasttext'],\n",
    "#    'word_embedding__size':[100,200,300],   \n",
    "#    'logit_clf_word2vec__C': range(0,10),\n",
    "#    'logit_clf_word2vec__class_weight':[{0: 0.9, 1: 0.1}, {0: 0.8, 1: 0.2}, {0: 0.7, 1: 0.3},None]\n",
    "#}\n",
    "\n",
    "#pipe_cv = RandomizedSearchCV(estimator = pipe,param_distributions = param_grid,\\\n",
    "#                                      cv =10, error_score = 0,n_iter = 10 , scoring = 'f1'\\\n",
    "#                                      ,return_train_score=True, n_jobs = 1)\n",
    "\n",
    "#pipe_cv.fit(data_train[\"Text\"],data_train['Target'])\n",
    "#print(pipe_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dumping pipeline after training\n",
    "#import joblib\n",
    "#joblib.dump(pipe_cv.best_estimator_, 'pipe_cv.pkl', compress = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIICatcher\n",
    "https://github.com/tokern/piicatcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'api',\n",
       " 'catalog',\n",
       " 'explorer',\n",
       " 'log_mixin',\n",
       " 'piitypes',\n",
       " 'scan_database',\n",
       " 'scanner']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import piicatcher\n",
    "from piicatcher import scan_database\n",
    "\n",
    "dir(piicatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import scanner from piicatcher\n",
    "\n",
    "from piicatcher import scanner\n",
    "from piicatcher import piitypes\n",
    "\n",
    "\"\"\"Different types of scanners for PII data\"\"\"\n",
    "import logging\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import spacy\n",
    "from commonregex import CommonRegex\n",
    "\n",
    "from piicatcher.piitypes import PiiTypes\n",
    "\n",
    "\n",
    "# pylint: disable=too-few-public-methods\n",
    "class Scanner(ABC):\n",
    "#\"\"\"Scanner abstract class that defines required methods\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def scan(self, text):\n",
    "#        \"\"\"Scan the text and return an array of PiiTypes that are found\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class RegexScanner(Scanner):\n",
    "#  \"\"\"A scanner that uses common regular expressions to find PII\"\"\"\n",
    "\n",
    "    def scan(self, text):\n",
    "        \"\"\"Scan the text and return an array of PiiTypes that are found\"\"\"\n",
    "        regex_result = CommonRegex(text)\n",
    "\n",
    "        types = []\n",
    "        if regex_result.phones:  # pylint: disable=no-member\n",
    "            types.append(PiiTypes.PHONE)\n",
    "        if regex_result.emails:  # pylint: disable=no-member\n",
    "            types.append(PiiTypes.EMAIL)\n",
    "        if regex_result.credit_cards:  # pylint: disable=no-member\n",
    "            types.append(PiiTypes.CREDIT_CARD)\n",
    "        if regex_result.street_addresses:  # pylint: disable=no-member\n",
    "            types.append(PiiTypes.ADDRESS)\n",
    "\n",
    "        #return types\n",
    "        return print(\"RegexScan:\", text,list(types))\n",
    "\n",
    "class NERScanner(Scanner):\n",
    "    \"\"\"A scanner that uses Spacy NER for entity recognition\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def scan(self, text):\n",
    "        \"\"\"Scan the text and return an array of PiiTypes that are found\"\"\"\n",
    "        logging.debug(\"Processing '%s'\", text)\n",
    "        doc = self.nlp(text)\n",
    "        types = set()\n",
    "        for ent in doc.ents:\n",
    "            logging.debug(\"Found %s\", ent.label_)\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                types.add(PiiTypes.PERSON)\n",
    "\n",
    "            if ent.label_ == \"GPE\":\n",
    "                types.add(PiiTypes.LOCATION)\n",
    "\n",
    "            if ent.label_ == \"DATE\":\n",
    "                types.add(PiiTypes.BIRTH_DATE)\n",
    "\n",
    "        logging.debug(\"PiiTypes are %s\", \",\".join(str(x) for x in list(types)))\n",
    "        #return list(types)\n",
    "        return print(\"NERScanner:\", text,list(types))\n",
    "\n",
    "\n",
    "class ColumnNameScanner(Scanner):\n",
    "    regex = {\n",
    "        PiiTypes.PERSON: re.compile(\n",
    "            \"^.*(firstname|fname|lastname|lname|\"\n",
    "            \"fullname|maidenname|_name|\"\n",
    "            \"nickname|name_suffix|name).*$\",\n",
    "            re.IGNORECASE,\n",
    "        ),\n",
    "        PiiTypes.EMAIL: re.compile(\"^.*(email|e-mail|mail).*$\", re.IGNORECASE),\n",
    "        PiiTypes.BIRTH_DATE: re.compile(\n",
    "            \"^.*(date_of_birth|dateofbirth|dob|\"\n",
    "            \"birthday|date_of_death|dateofdeath).*$\",\n",
    "            re.IGNORECASE,\n",
    "        ),\n",
    "        PiiTypes.GENDER: re.compile(\"^.*(gender).*$\", re.IGNORECASE),\n",
    "        PiiTypes.NATIONALITY: re.compile(\"^.*(nationality).*$\", re.IGNORECASE),\n",
    "        PiiTypes.ADDRESS: re.compile(\n",
    "            \"^.*(address|city|state|county|country|\" \"zipcode|postal|zone|borough).*$\",\n",
    "            re.IGNORECASE,\n",
    "        ),\n",
    "        PiiTypes.USER_NAME: re.compile(\"^.*user(id|name|).*$\", re.IGNORECASE),\n",
    "        PiiTypes.PASSWORD: re.compile(\"^.*pass.*$\", re.IGNORECASE),\n",
    "        PiiTypes.SSN: re.compile(\"^.*(ssn|social).*$\", re.IGNORECASE),\n",
    "    }\n",
    "\n",
    "    def scan(self, text):\n",
    "        types = set()\n",
    "        for pii_type in self.regex:\n",
    "            if self.regex[pii_type].match(text) is not None:\n",
    "                types.add(pii_type)\n",
    "\n",
    "        return print(\"ColumnNameScanner:\", text,list(types))\n",
    "\n",
    "#print(RegexScanner().scan(\"610-504-0413\")) \n",
    "#NERScanner().scan(\"Tommy is out. Jason is in Great Britain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PII_df.txt') as f:\n",
    "  print(piicatcher.scan_file_object(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnNameScanner: My phone number is 610-504-0413 []\n",
      "NERScanner: My phone number is 610-504-0413 []\n",
      "RegexScan: My phone number is 610-504-0413 [<PiiTypes.PHONE: 3>]\n"
     ]
    }
   ],
   "source": [
    "scan_type = [ColumnNameScanner(), NERScanner(), RegexScanner()]\n",
    "\n",
    "for Scanner in scan_type:\n",
    "    Scanner.scan(text = 'My phone number is 610-504-0413')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIIAnalyzer\n",
    "https://gitlab.math.ubc.ca/tomyerex/piianalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "\n",
    "stanford_ner_dir = 'stanford-ner/' # download file from here https://nlp.stanford.edu/software/CRF-NER.shtml#Download and then set directory where the file is\n",
    "eng_model_filename= stanford_ner_dir + 'classifiers/english.conll.4class.distsim.crf.ser.gz'\n",
    "my_path_to_jar= stanford_ner_dir + 'stanford-ner.jar'\n",
    "\n",
    "st = StanfordNERTagger(model_filename=eng_model_filename, path_to_jar=my_path_to_jar) \n",
    "#st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) #example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from commonregex import CommonRegex\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "\n",
    "\n",
    "class PiiAnalyzer(object):\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.parser = CommonRegex()\n",
    "        #self.standford_ner = StanfordNERTagger('classifiers/english.conll.4class.distsim.crf.ser.gz')\n",
    "        self.standford_ner = st\n",
    "\n",
    "    def analysis(self):\n",
    "        people = []\n",
    "        organizations = []\n",
    "        locations = []\n",
    "        emails = []\n",
    "        phone_numbers = []\n",
    "        street_addresses = []\n",
    "        credit_cards = []\n",
    "        ips = []\n",
    "        data = []\n",
    "\n",
    "\n",
    "        with open(self.filepath, newline='') as filedata:\n",
    "            reader = csv.reader(filedata)\n",
    "\n",
    "            for row in reader:\n",
    "                data.extend(row)\n",
    "                for text in row:\n",
    "                    emails.extend(self.parser.emails(text))\n",
    "                    phone_numbers.extend(self.parser.phones(\"\".join(text.split())))\n",
    "                    street_addresses.extend(self.parser.street_addresses(text))\n",
    "                    credit_cards.extend(self.parser.credit_cards(text))\n",
    "                    ips.extend(self.parser.ips(text))\n",
    " \n",
    "\n",
    "        for title, tag in self.standford_ner.tag(set(data)):\n",
    "            if tag == 'PERSON':\n",
    "                people.append(title)\n",
    "            if tag == 'LOCATION':\n",
    "                locations.append(title)\n",
    "            if tag == 'ORGANIZATION':\n",
    "                organizations.append(title)\n",
    "\n",
    "        return {'people': people, 'locations': locations, 'organizations': organizations,\n",
    "                'emails': emails, 'phone_numbers': phone_numbers, 'street_addresses': street_addresses,\n",
    "                'credit_cards': credit_cards, 'ips': ips\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': ['Sam', 'Country', 'Roberts'],\n",
       " 'locations': ['United', 'States', 'Benin'],\n",
       " 'organizations': ['Michael', 'Email'],\n",
       " 'emails': ['mroberts2@pbs.org',\n",
       "  'awagner3@altervista.org',\n",
       "  'mwagner4@zimbio.com'],\n",
       " 'phone_numbers': ['0796477389', '9-(937)171-5306', '4-(374)794-1813'],\n",
       " 'street_addresses': [],\n",
       " 'credit_cards': [],\n",
       " 'ips': ['72.141.150.39', '247.65.204.78', '202.9.208.160']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PiiAnalyzer('pii.csv').analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
